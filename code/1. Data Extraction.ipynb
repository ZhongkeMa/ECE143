{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "The following functions are used to extract raw data from the various sources and store them in offline text and json files.\n",
    "\n",
    "`LeetcodeDataExtractor` : \n",
    "    This class defines the basic methods required to login to leetcode with specific user credentials. \n",
    "    \n",
    "`problem_stat_scraper` : \n",
    "    This function in the above class extracts all problem stats and dumps it into a json file in the following format - ../data/problems_{user}.json  \n",
    "    \n",
    "`submission_scraper` : \n",
    "    This function in the above class extracts all submission stats and dumps it into a json file in the following format - ../data/submissions_{user}.json  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "from requests_toolbelt import MultipartEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeetcodeDataExtractor():\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.csrftoken = ''\n",
    "        self.is_login = False\n",
    "\n",
    "    def get_csrftoken(self):\n",
    "        url = 'https://leetcode.com'\n",
    "        cookies = self.session.get(url).cookies\n",
    "        for cookie in cookies:\n",
    "            if cookie.name == 'csrftoken':\n",
    "                self.csrftoken = cookie.value\n",
    "                break\n",
    "\n",
    "    def login(self, username, password):\n",
    "        url = \"https://leetcode.com/accounts/login\"\n",
    "\n",
    "        params_data = {\n",
    "            'csrfmiddlewaretoken': self.csrftoken,\n",
    "            'login': username,\n",
    "            'password':password,\n",
    "            'next': 'problems'\n",
    "        }\n",
    "        headers = {'User-Agent': user_agent, 'Connection': 'keep-alive', 'Referer': 'https://leetcode.com/accounts/login/',\n",
    "        \"origin\": \"https://leetcode.com\"}\n",
    "        m = MultipartEncoder(params_data)\n",
    "\n",
    "        headers['Content-Type'] = m.content_type\n",
    "        self.session.post(url, headers = headers, data = m, timeout = 10, allow_redirects = False)\n",
    "        self.is_login = self.session.cookies.get('LEETCODE_SESSION') != None\n",
    "        return self.is_login\n",
    "\n",
    "    def problem_stat_scraper(self,user,credentials):\n",
    "        '''\n",
    "        Function to extract all content from the problemset api of a user and dump the content into a json file\n",
    "        Args:\n",
    "            user: Name of current user\n",
    "            credentials: username and password of current leetcode login\n",
    "        Returns:\n",
    "            None\n",
    "            Dumps content into ../data/problems_{user}.json\n",
    "        '''\n",
    "        username = credentials[0]\n",
    "        password = credentials[1]\n",
    "        is_login = self.login(username,password)\n",
    "\n",
    "        #breakpoint()\n",
    "        headers = {'User-Agent': user_agent, 'Connection': 'keep-alive'}\n",
    "        page_link = \"https://leetcode.com/api/problems/all/\"\n",
    "        resp = self.session.get(page_link, headers = headers, timeout = 10)\n",
    "        page_content = json.loads(resp.content.decode('utf-8'))\n",
    "\n",
    "        with open(f'../data/problems_{user}.json', 'w') as file_json:\n",
    "            json.dump(page_content, file_json, indent=2, sort_keys=False)\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        #pp.pprint(page_content)\n",
    "        #print(page_content.keys())\n",
    "        print(f'{user} Problem Scraper success')\n",
    "\n",
    "        return page_content\n",
    "\n",
    "    def submission_scraper(self,user,credentials):\n",
    "        '''\n",
    "        Function to extract all content from the submissions api of a user and dump the content into a json file\n",
    "        Args:\n",
    "            user: Name of current user\n",
    "            credentials: username and password of current leetcode login\n",
    "        Returns:\n",
    "            None\n",
    "            Dumps content into ../data/submissions_{user}.json\n",
    "        '''\n",
    "        username = credentials[0]\n",
    "        password = credentials[1]\n",
    "        is_login = self.login(username,password)\n",
    "\n",
    "        #breakpoint()\n",
    "        headers = {'User-Agent': user_agent, 'Connection': 'keep-alive'}\n",
    "        page_link = \"https://leetcode.com/api/submissions/\"\n",
    "        resp = self.session.get(page_link, headers = headers, timeout = 10)\n",
    "        page_content = json.loads(resp.content.decode('utf-8'))\n",
    "\n",
    "        with open(f'../data/submissions_{user}.json', 'w') as file_json:\n",
    "            json.dump(page_content, file_json, indent=2, sort_keys=False)\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        #pp.pprint(page_content)\n",
    "        #print(page_content.keys())\n",
    "        print(f'{user} Submission Scraper success')\n",
    "\n",
    "        return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_leetcode_tags():\n",
    "    '''\n",
    "    Function to extract all the topic tags from leetcode problems page\n",
    "    Args: None\n",
    "    Returns:\n",
    "        topic_tags: list of all topic tags\n",
    "    '''\n",
    "\n",
    "    page_link = 'https://leetcode.com/problemset/all/'\n",
    "    page_response = requests.get(page_link, timeout=5)\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "    #if we want current_topics only\n",
    "    current_topics = page_content.findAll('div',attrs={\"class\":\"tags tags-fade\", \"id\":\"current-topic-tags\"})\n",
    "    topic_elems=current_topics[0].findAll('span', attrs={'class':'text-sm text-gray'})\n",
    "    for elem in topic_elems:\n",
    "        #print(elem.text.strip())\n",
    "        pass\n",
    "\n",
    "    #if we want all topic tags\n",
    "    all_topics = page_content.find('span', attrs={'class':'hide', 'id':'all-topic-tags'}).findAll('span',attrs={'class':'text-sm text-gray'})\n",
    "    topic_tags = []\n",
    "    for elem in all_topics:\n",
    "        topic_tags.append(elem.text.strip())\n",
    "\n",
    "    return topic_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = r'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'\n",
    "\n",
    "def dataset_collect():\n",
    "    with open(f'../data/credentials.json', 'r') as file_json:\n",
    "        credentials = json.load(file_json)\n",
    "    for names in credentials.keys():\n",
    "        trial = LeetcodeDataExtractor()\n",
    "        trial.get_csrftoken()\n",
    "        trial.problem_stat_scraper(names, credentials[names])\n",
    "        trial.submission_scraper(names, credentials[names])\n",
    "        del trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Problem Scraper success\n",
      "Alice Submission Scraper success\n",
      "Bob Problem Scraper success\n",
      "Bob Submission Scraper success\n",
      "Carl Problem Scraper success\n",
      "Carl Submission Scraper success\n",
      "Dave Problem Scraper success\n",
      "Dave Submission Scraper success\n",
      "Eve Problem Scraper success\n",
      "Eve Submission Scraper success\n",
      "Frank Problem Scraper success\n",
      "Frank Submission Scraper success\n"
     ]
    }
   ],
   "source": [
    "dataset_collect()\n",
    "topic_tags = extract_leetcode_tags()\n",
    "with open('../data/tags.txt', 'w') as file:\n",
    "    [file.write(f'{elem}\\n') for elem in topic_tags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
